# -*- coding: utf-8 -*-
"""M23MA2006_prob4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ICQvgV-wpukgxPmouD_N_h1nGA4EZD2n
"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report

# STEP 1: Dataset load kar rahe hain
# Text file me har line pe label aur content hai
# (SPORTS ya POLITICS)

news_df = pd.read_csv(
    "indian_news_sports_politics.txt",
    sep="\t",
    header=None,
    names=["label", "content"]
)

# Sirf required classes rakh rahe hain
# Agar aur koi label ho to usko ignore kar diya jayega
news_df = news_df[news_df["label"].isin(["SPORTS", "POLITICS"])]

documents = news_df["content"].values
targets = news_df["label"].values


# STEP 2: Trainâ€“test split
# 80% data training ke liye
# 20% data testing ke liye
# stratify use kiya hai taaki class balance bana rahe

X_tr, X_te, y_tr, y_te = train_test_split(
    documents,
    targets,
    test_size=0.2,
    random_state=42,
    stratify=targets
)


# STEP 3: Bag of Words representation
# Sirf word frequency count karta hai
# Word order ya context yaha consider nahi hota

bow_vectorizer = CountVectorizer(
    lowercase=True,
    stop_words="english",
    min_df=2
)

X_tr_bow = bow_vectorizer.fit_transform(X_tr)
X_te_bow = bow_vectorizer.transform(X_te)

# LIMITATION:
# Bag of Words semantic meaning capture nahi karta,
# sirf words ki count pe kaam karta hai


# STEP 4: TF-IDF representation
# Common words ka weight kam aur
# important words ka weight zyada hota hai

tfidf_vectorizer = TfidfVectorizer(
    lowercase=True,
    stop_words="english",
    min_df=2
)

X_tr_tfidf = tfidf_vectorizer.fit_transform(X_tr)
X_te_tfidf = tfidf_vectorizer.transform(X_te)

# LIMITATION:
# TF-IDF bhi word order ko ignore karta hai,
# sentence ka context fully capture nahi hota


# STEP 5: TF-IDF with n-grams
# Unigrams + bigrams use kar rahe hain
# Isse thoda context better milta hai

tfidf_ngram_vectorizer = TfidfVectorizer(
    lowercase=True,
    stop_words="english",
    min_df=2,
    ngram_range=(1, 2)
)

X_tr_ng = tfidf_ngram_vectorizer.fit_transform(X_tr)
X_te_ng = tfidf_ngram_vectorizer.transform(X_te)

# LIMITATION:
# N-grams se feature size kaafi badh jata hai,
# jo computation ko slow bana sakta hai


# STEP 6: Naive Bayes model
# Simple probabilistic classifier hai
# Word independence assume karta hai

nb_model = MultinomialNB()
nb_model.fit(X_tr_tfidf, y_tr)
nb_predictions = nb_model.predict(X_te_tfidf)

print("Naive Bayes Result:")
print("Accuracy =", accuracy_score(y_te, nb_predictions))
print(classification_report(y_te, nb_predictions))

# LIMITATION:
# Naive Bayes real language me independence assumption
# ki wajah se kabhi kabhi galat predict kar deta hai


# STEP 7: Logistic Regression model
# Linear classifier hai
# Decision boundary learn karta hai

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_tr_tfidf, y_tr)
lr_predictions = lr_model.predict(X_te_tfidf)

print("Logistic Regression Result:")
print("Accuracy =", accuracy_score(y_te, lr_predictions))
print(classification_report(y_te, lr_predictions))

# LIMITATION:
# Agar data non-linear ho,
# to logistic regression utna acha perform nahi karta


# STEP 8: Support Vector Machine model
# Maximum margin find karta hai between classes
# High dimensional data me usually acha kaam karta hai

svm_model = LinearSVC()
svm_model.fit(X_tr_tfidf, y_tr)
svm_predictions = svm_model.predict(X_te_tfidf)

print("Linear SVM Result:")
print("Accuracy =", accuracy_score(y_te, svm_predictions))
print(classification_report(y_te, svm_predictions))

# LIMITATION:
# SVM ka training time large dataset pe zyada ho sakta hai,
# aur hyperparameters tuning ke bina best result nahi milta